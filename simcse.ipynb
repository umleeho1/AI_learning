{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN89FhNmClm3qTh6TGwax9m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umleeho1/AI_learning/blob/main/simcse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "hugging face 데이터셋 다운"
      ],
      "metadata": {
        "id": "mCilTJ1wzHP5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbIemNaat3lo"
      },
      "outputs": [],
      "source": [
        "!pip cache purge\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install python3.7\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1\n",
        "!sudo update-alternatives --config python3\n",
        "!sudo apt install python3-pip\n",
        "!sudo apt install python3.7-distutils"
      ],
      "metadata": {
        "id": "WQRubAGm8vhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "IOEQNZDCy8Px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/princeton-nlp/SimCSE/archive/refs/tags/0.4.tar.gz"
      ],
      "metadata": {
        "id": "pO0OlVaT9BSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzvf 0.4.tar.gz"
      ],
      "metadata": {
        "id": "ATUxQtKh4RD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.version)"
      ],
      "metadata": {
        "id": "A64lTXu95ScU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show transformers"
      ],
      "metadata": {
        "id": "-8yCRzj732sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "vfWqnzZo46nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#hugging face와 연결"
      ],
      "metadata": {
        "id": "K2bHVCUG07wA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Convert SimCSE's checkpoints to Huggingface style.\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--path\", type=str, help=\"Path of SimCSE checkpoint folder\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(\"SimCSE checkpoint -> Huggingface checkpoint for {}\".format(args.path))\n",
        "\n",
        "    state_dict = torch.load(os.path.join(args.path, \"pytorch_model.bin\"), map_location=torch.device(\"cpu\"))\n",
        "    new_state_dict = {}\n",
        "    for key, param in state_dict.items():\n",
        "        # Replace \"mlp\" to \"pooler\"\n",
        "        if \"mlp\" in key:\n",
        "            key = key.replace(\"mlp\", \"pooler\")\n",
        "\n",
        "        # Delete \"bert\" or \"roberta\" prefix\n",
        "        if \"bert.\" in key:\n",
        "            key = key.replace(\"bert.\", \"\")\n",
        "        if \"roberta.\" in key:\n",
        "            key = key.replace(\"roberta.\", \"\")\n",
        "\n",
        "        new_state_dict[key] = param\n",
        "\n",
        "    torch.save(new_state_dict, os.path.join(args.path, \"pytorch_model.bin\"))\n",
        "\n",
        "    # Change architectures in config.json\n",
        "    config = json.load(open(os.path.join(args.path, \"config.json\")))\n",
        "    for i in range(len(config[\"architectures\"])):\n",
        "        config[\"architectures\"][i] = config[\"architectures\"][i].replace(\"ForCL\", \"Model\")\n",
        "    json.dump(config, open(os.path.join(args.path, \"config.json\"), \"w\"), indent=2)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "zz79APu21A74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train\n",
        "**굵은 텍스트**"
      ],
      "metadata": {
        "id": "xSiZc7p5zELf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, Union, List, Dict, Tuple\n",
        "import torch\n",
        "import collections\n",
        "import random\n",
        "# datasetload\n",
        "\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    MODEL_FOR_MASKED_LM_MAPPING,\n",
        "    AutoConfig,\n",
        "    AutoModelForMaskedLM,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    DataCollatorWithPadding,\n",
        "    HfArgumentParser,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    default_data_collator,\n",
        "    set_seed,\n",
        "    EvalPrediction,\n",
        "    BertModel,\n",
        "    BertForPreTraining,\n",
        "    RobertaModel\n",
        ")\n",
        "from transformers.tokenization_utils_base import BatchEncoding, PaddingStrategy, PreTrainedTokenizerBase\n",
        "from transformers.trainer_utils import is_main_process\n",
        "from transformers.data.data_collator import DataCollatorForLanguageModeling\n",
        "from transformers.file_utils import cached_property, is_torch_available, is_torch_tpu_available\n"
      ],
      "metadata": {
        "id": "firvJ8-c20LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from simcse.models import RobertaForCL, BertForCL\n",
        "from simcse.trainers import CLTrainer"
      ],
      "metadata": {
        "id": "E5m87wHB5cou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# datasetload\n",
        "\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "jRpQ34zW4rFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "MODEL_CONFIG_CLASSES = list(MODEL_FOR_MASKED_LM_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Huggingface's original arguments\n",
        "    model_name_or_path: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"The model checkpoint for weights initialization.\"\n",
        "            \"Don't set if you want to train a model from scratch.\"\n",
        "        },\n",
        "    )\n",
        "    model_type: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
        "    )\n",
        "    config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
        "    )\n",
        "    tokenizer_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
        "    )\n",
        "    use_fast_tokenizer: bool = field(\n",
        "        default=True,\n",
        "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
        "    )\n",
        "    model_revision: str = field(\n",
        "        default=\"main\",\n",
        "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
        "    )\n",
        "    use_auth_token: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
        "            \"with private models).\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # SimCSE's arguments\n",
        "    temp: float = field(\n",
        "        default=0.05,\n",
        "        metadata={\n",
        "            \"help\": \"Temperature for softmax.\"\n",
        "        }\n",
        "    )\n",
        "    pooler_type: str = field(\n",
        "        default=\"cls\",\n",
        "        metadata={\n",
        "            \"help\": \"What kind of pooler to use (cls, cls_before_pooler, avg, avg_top2, avg_first_last).\"\n",
        "        }\n",
        "    )\n",
        "    hard_negative_weight: float = field(\n",
        "        default=0,\n",
        "        metadata={\n",
        "            \"help\": \"The **logit** of weight for hard negatives (only effective if hard negatives are used).\"\n",
        "        }\n",
        "    )\n",
        "    do_mlm: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": \"Whether to use MLM auxiliary objective.\"\n",
        "        }\n",
        "    )\n",
        "    mlm_weight: float = field(\n",
        "        default=0.1,\n",
        "        metadata={\n",
        "            \"help\": \"Weight for MLM auxiliary objective (only effective if --do_mlm).\"\n",
        "        }\n",
        "    )\n",
        "    mlp_only_train: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": \"Use MLP only during training\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    \"\"\"\n",
        "\n",
        "    # Huggingface's original arguments.\n",
        "\n",
        "    dataset_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
        "    )\n",
        "    dataset_config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
        "    )\n",
        "    # 검증데이터가없을시 dafault값에따라 학습데이터중 일부를 사용해 검증데이터로함\n",
        "    validation_split_percentage: Optional[int] = field(\n",
        "        default=5,\n",
        "        metadata={\n",
        "            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n",
        "        },\n",
        "    )\n",
        "    preprocessing_num_workers: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
        "    )\n",
        "\n",
        "    # SimCSE's arguments\n",
        "    train_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The training data file (.txt or .csv).\"}\n",
        "    )\n",
        "    max_seq_length: Optional[int] = field(\n",
        "        default=32,\n",
        "        metadata={\n",
        "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "            \"than this will be truncated.\"\n",
        "        },\n",
        "    )\n",
        "    pad_to_max_length: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n",
        "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n",
        "        },\n",
        "    )\n",
        "    mlm_probability: float = field(\n",
        "        default=0.15,\n",
        "        metadata={\"help\": \"Ratio of tokens to mask for MLM (only effective if --do_mlm)\"}\n",
        "    )\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
        "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
        "        else:\n",
        "            if self.train_file is not None:\n",
        "                extension = self.train_file.split(\".\")[-1]\n",
        "                assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, a json or a txt file.\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class OurTrainingArguments(TrainingArguments):\n",
        "    # Evaluation\n",
        "    ## By default, we evaluate STS (dev) during training (for selecting best checkpoints) and evaluate\n",
        "    ## both STS and transfer tasks (dev) at the end of training. Using --eval_transfer will allow evaluating\n",
        "    ## both STS and transfer tasks (dev) during training.\n",
        "    eval_transfer: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Evaluate transfer task dev sets (in validation).\"}\n",
        "    )\n",
        "\n",
        "    @cached_property\n",
        "    @torch_required\n",
        "    def _setup_devices(self) -> \"torch.device\":\n",
        "        logger.info(\"PyTorch: setting up devices\")\n",
        "        if self.no_cuda:\n",
        "            device = torch.device(\"cpu\")\n",
        "            self._n_gpu = 0\n",
        "        elif is_torch_tpu_available():\n",
        "            import torch_xla.core.xla_model as xm\n",
        "            device = xm.xla_device()\n",
        "            self._n_gpu = 0\n",
        "        elif self.local_rank == -1:\n",
        "            # if n_gpu is > 1 we'll use nn.DataParallel.\n",
        "            # If you only want to use a specific subset of GPUs use `CUDA_VISIBLE_DEVICES=0`\n",
        "            # Explicitly set CUDA to the first (index 0) CUDA device, otherwise `set_device` will\n",
        "            # trigger an error that a device index is missing. Index 0 takes into account the\n",
        "            # GPUs available in the environment, so `CUDA_VISIBLE_DEVICES=1,2` with `cuda:0`\n",
        "            # will use the first GPU in that env, i.e. GPU#1\n",
        "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "            # Sometimes the line in the postinit has not been run before we end up here, so just checking we're not at\n",
        "            # the default value.\n",
        "            self._n_gpu = torch.cuda.device_count()\n",
        "        else:\n",
        "            # Here, we'll use torch.distributed.\n",
        "            # Initializes the distributed backend which will take care of synchronizing nodes/GPUs\n",
        "            #\n",
        "            # deepspeed performs its own DDP internally, and requires the program to be started with:\n",
        "            # deepspeed  ./program.py\n",
        "            # rather than:\n",
        "            # python -m torch.distributed.launch --nproc_per_node=2 ./program.py\n",
        "            if self.deepspeed:\n",
        "                from .integrations import is_deepspeed_available\n",
        "\n",
        "                if not is_deepspeed_available():\n",
        "                    raise ImportError(\"--deepspeed requires deepspeed: `pip install deepspeed`.\")\n",
        "                import deepspeed\n",
        "\n",
        "                deepspeed.init_distributed()\n",
        "            else:\n",
        "                torch.distributed.init_process_group(backend=\"nccl\")\n",
        "            device = torch.device(\"cuda\", self.local_rank)\n",
        "            self._n_gpu = 1\n",
        "\n",
        "        if device.type == \"cuda\":\n",
        "            torch.cuda.set_device(device)\n",
        "\n",
        "        return device\n",
        "\n",
        "\n",
        "def main():\n",
        "    # See all possible arguments in src/transformers/training_args.py\n",
        "    # or by passing the --help flag to this script.\n",
        "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
        "\n",
        "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, OurTrainingArguments))\n",
        "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
        "        # If we pass only one argument to the script and it's the path to a json file,\n",
        "        # let's parse it to get our arguments.\n",
        "        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
        "    else:\n",
        "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
        "\n",
        "    if (\n",
        "        os.path.exists(training_args.output_dir)\n",
        "        and os.listdir(training_args.output_dir)\n",
        "        and training_args.do_train\n",
        "        and not training_args.overwrite_output_dir\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            f\"Output directory ({training_args.output_dir}) already exists and is not empty.\"\n",
        "            \"Use --overwrite_output_dir to overcome.\"\n",
        "        )\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO if is_main_process(training_args.local_rank) else logging.WARN,\n",
        "    )\n",
        "\n",
        "    # Log on each process the small summary:\n",
        "    logger.warning(\n",
        "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
        "        + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
        "    )\n",
        "    # Set the verbosity to info of the Transformers logger (on main process only):\n",
        "    if is_main_process(training_args.local_rank):\n",
        "        transformers.utils.logging.set_verbosity_info()\n",
        "        transformers.utils.logging.enable_default_handler()\n",
        "        transformers.utils.logging.enable_explicit_format()\n",
        "    logger.info(\"Training/evaluation parameters %s\", training_args)\n",
        "\n",
        "    # Set seed before initializing model.\n",
        "    set_seed(training_args.seed)\n",
        "\n",
        "    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n",
        "    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
        "    # (the dataset will be downloaded automatically from the datasets Hub\n",
        "    #\n",
        "    # For CSV/JSON files, this script will use the column called 'text' or the first column. You can easily tweak this\n",
        "    # behavior (see below)\n",
        "    #\n",
        "    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
        "    # download the dataset.\n",
        "    data_files = {}\n",
        "    if data_args.train_file is not None:\n",
        "        data_files[\"train\"] = data_args.train_file\n",
        "    extension = data_args.train_file.split(\".\")[-1]\n",
        "    if extension == \"txt\":\n",
        "        extension = \"text\"\n",
        "    if extension == \"csv\":\n",
        "        datasets = load_dataset(extension, data_files=data_files, cache_dir=\"./data/\", delimiter=\"\\t\" if \"tsv\" in data_args.train_file else \",\")\n",
        "    else:\n",
        "        datasets = load_dataset(extension, data_files=data_files, cache_dir=\"./data/\")\n",
        "\n",
        "    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
        "    # https://huggingface.co/docs/datasets/loading_datasets.html.\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    #\n",
        "    # Distributed training:\n",
        "    # The .from_pretrained methods guarantee that only one local process can concurrently\n",
        "    # download model & vocab.\n",
        "    config_kwargs = {\n",
        "        \"cache_dir\": model_args.cache_dir,\n",
        "        \"revision\": model_args.model_revision,\n",
        "        \"use_auth_token\": True if model_args.use_auth_token else None,\n",
        "    }\n",
        "    if model_args.config_name:\n",
        "        config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)\n",
        "    elif model_args.model_name_or_path:\n",
        "        config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n",
        "    else:\n",
        "        config = CONFIG_MAPPING[model_args.model_type]()\n",
        "        logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
        "\n",
        "    tokenizer_kwargs = {\n",
        "        \"cache_dir\": model_args.cache_dir,\n",
        "        \"use_fast\": model_args.use_fast_tokenizer,\n",
        "        \"revision\": model_args.model_revision,\n",
        "        \"use_auth_token\": True if model_args.use_auth_token else None,\n",
        "    }\n",
        "    if model_args.tokenizer_name:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)\n",
        "    elif model_args.model_name_or_path:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, **tokenizer_kwargs)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
        "            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
        "        )\n",
        "\n",
        "    if model_args.model_name_or_path:\n",
        "        if 'roberta' in model_args.model_name_or_path:\n",
        "            model = RobertaForCL.from_pretrained(\n",
        "                model_args.model_name_or_path,\n",
        "                from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
        "                config=config,\n",
        "                cache_dir=model_args.cache_dir,\n",
        "                revision=model_args.model_revision,\n",
        "                use_auth_token=True if model_args.use_auth_token else None,\n",
        "                model_args=model_args\n",
        "            )\n",
        "        elif 'bert' in model_args.model_name_or_path:\n",
        "            model = BertForCL.from_pretrained(\n",
        "                model_args.model_name_or_path,\n",
        "                from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
        "                config=config,\n",
        "                cache_dir=model_args.cache_dir,\n",
        "                revision=model_args.model_revision,\n",
        "                use_auth_token=True if model_args.use_auth_token else None,\n",
        "                model_args=model_args\n",
        "            )\n",
        "            if model_args.do_mlm:\n",
        "                pretrained_model = BertForPreTraining.from_pretrained(model_args.model_name_or_path)\n",
        "                model.lm_head.load_state_dict(pretrained_model.cls.predictions.state_dict())\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "        logger.info(\"Training new model from scratch\")\n",
        "        model = AutoModelForMaskedLM.from_config(config)\n",
        "\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    # Prepare features\n",
        "    column_names = datasets[\"train\"].column_names\n",
        "    sent2_cname = None\n",
        "    if len(column_names) == 2:\n",
        "        # Pair datasets\n",
        "        sent0_cname = column_names[0]\n",
        "        sent1_cname = column_names[1]\n",
        "    elif len(column_names) == 3:\n",
        "        # Pair datasets with hard negatives\n",
        "        sent0_cname = column_names[0]\n",
        "        sent1_cname = column_names[1]\n",
        "        sent2_cname = column_names[2]\n",
        "    elif len(column_names) == 1:\n",
        "        # Unsupervised datasets\n",
        "        sent0_cname = column_names[0]\n",
        "        sent1_cname = column_names[0]\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def prepare_features(examples):\n",
        "        # padding = longest (default)\n",
        "        #   If no sentence in the batch exceed the max length, then use\n",
        "        #   the max sentence length in the batch, otherwise use the\n",
        "        #   max sentence length in the argument and truncate those that\n",
        "        #   exceed the max length.\n",
        "        # padding = max_length (when pad_to_max_length, for pressure test)\n",
        "        #   All sentences are padded/truncated to data_args.max_seq_length.\n",
        "        total = len(examples[sent0_cname])\n",
        "\n",
        "        # Avoid \"None\" fields\n",
        "        for idx in range(total):\n",
        "            if examples[sent0_cname][idx] is None:\n",
        "                examples[sent0_cname][idx] = \" \"\n",
        "            if examples[sent1_cname][idx] is None:\n",
        "                examples[sent1_cname][idx] = \" \"\n",
        "\n",
        "        sentences = examples[sent0_cname] + examples[sent1_cname]\n",
        "\n",
        "        # If hard negative exists\n",
        "        if sent2_cname is not None:\n",
        "            for idx in range(total):\n",
        "                if examples[sent2_cname][idx] is None:\n",
        "                    examples[sent2_cname][idx] = \" \"\n",
        "            sentences += examples[sent2_cname]\n",
        "\n",
        "        sent_features = tokenizer(\n",
        "            sentences,\n",
        "            max_length=data_args.max_seq_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\" if data_args.pad_to_max_length else False,\n",
        "        )\n",
        "\n",
        "        features = {}\n",
        "        if sent2_cname is not None:\n",
        "            for key in sent_features:\n",
        "                features[key] = [[sent_features[key][i], sent_features[key][i+total], sent_features[key][i+total*2]] for i in range(total)]\n",
        "        else:\n",
        "            for key in sent_features:\n",
        "                features[key] = [[sent_features[key][i], sent_features[key][i+total]] for i in range(total)]\n",
        "\n",
        "        return features\n",
        "\n",
        "    if training_args.do_train:\n",
        "        train_dataset = datasets[\"train\"].map(\n",
        "            prepare_features,\n",
        "            batched=True,\n",
        "            num_proc=data_args.preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "        )\n",
        "\n",
        "    # Data collator\n",
        "    @dataclass\n",
        "    class OurDataCollatorWithPadding:\n",
        "\n",
        "        tokenizer: PreTrainedTokenizerBase\n",
        "        padding: Union[bool, str, PaddingStrategy] = True\n",
        "        max_length: Optional[int] = None\n",
        "        pad_to_multiple_of: Optional[int] = None\n",
        "        mlm: bool = True\n",
        "        mlm_probability: float = data_args.mlm_probability\n",
        "\n",
        "        def __call__(self, features: List[Dict[str, Union[List[int], List[List[int]], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "            special_keys = ['input_ids', 'attention_mask', 'token_type_ids', 'mlm_input_ids', 'mlm_labels']\n",
        "            bs = len(features)\n",
        "            if bs > 0:\n",
        "                num_sent = len(features[0]['input_ids'])\n",
        "            else:\n",
        "                return\n",
        "            flat_features = []\n",
        "            for feature in features:\n",
        "                for i in range(num_sent):\n",
        "                    flat_features.append({k: feature[k][i] if k in special_keys else feature[k] for k in feature})\n",
        "\n",
        "            batch = self.tokenizer.pad(\n",
        "                flat_features,\n",
        "                padding=self.padding,\n",
        "                max_length=self.max_length,\n",
        "                pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "            if model_args.do_mlm:\n",
        "                batch[\"mlm_input_ids\"], batch[\"mlm_labels\"] = self.mask_tokens(batch[\"input_ids\"])\n",
        "\n",
        "            batch = {k: batch[k].view(bs, num_sent, -1) if k in special_keys else batch[k].view(bs, num_sent, -1)[:, 0] for k in batch}\n",
        "\n",
        "            if \"label\" in batch:\n",
        "                batch[\"labels\"] = batch[\"label\"]\n",
        "                del batch[\"label\"]\n",
        "            if \"label_ids\" in batch:\n",
        "                batch[\"labels\"] = batch[\"label_ids\"]\n",
        "                del batch[\"label_ids\"]\n",
        "\n",
        "            return batch\n",
        "\n",
        "        def mask_tokens(\n",
        "            self, inputs: torch.Tensor, special_tokens_mask: Optional[torch.Tensor] = None\n",
        "        ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "            \"\"\"\n",
        "            Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
        "            \"\"\"\n",
        "            inputs = inputs.clone()\n",
        "            labels = inputs.clone()\n",
        "            # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
        "            probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
        "            if special_tokens_mask is None:\n",
        "                special_tokens_mask = [\n",
        "                    self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
        "                ]\n",
        "                special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
        "            else:\n",
        "                special_tokens_mask = special_tokens_mask.bool()\n",
        "\n",
        "            probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
        "            masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "            labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
        "\n",
        "            # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "            indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
        "            inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
        "\n",
        "            # 10% of the time, we replace masked input tokens with random word\n",
        "            indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "            random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
        "            inputs[indices_random] = random_words[indices_random]\n",
        "\n",
        "            # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
        "            return inputs, labels\n",
        "\n",
        "    data_collator = default_data_collator if data_args.pad_to_max_length else OurDataCollatorWithPadding(tokenizer)\n",
        "\n",
        "    trainer = CLTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset if training_args.do_train else None,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "    trainer.model_args = model_args\n",
        "\n",
        "    # Training\n",
        "    if training_args.do_train:\n",
        "        model_path = (\n",
        "            model_args.model_name_or_path\n",
        "            if (model_args.model_name_or_path is not None and os.path.isdir(model_args.model_name_or_path))\n",
        "            else None\n",
        "        )\n",
        "        train_result = trainer.train(model_path=model_path)\n",
        "        trainer.save_model()  # Saves the tokenizer too for easy upload\n",
        "\n",
        "        output_train_file = os.path.join(training_args.output_dir, \"train_results.txt\")\n",
        "        if trainer.is_world_process_zero():\n",
        "            with open(output_train_file, \"w\") as writer:\n",
        "                logger.info(\"***** Train results *****\")\n",
        "                for key, value in sorted(train_result.metrics.items()):\n",
        "                    logger.info(f\"  {key} = {value}\")\n",
        "                    writer.write(f\"{key} = {value}\\n\")\n",
        "\n",
        "            # Need to save the state, since Trainer.save_model saves only the tokenizer with the model\n",
        "            trainer.state.save_to_json(os.path.join(training_args.output_dir, \"trainer_state.json\"))\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if training_args.do_eval:\n",
        "        logger.info(\"*** Evaluate ***\")\n",
        "        results = trainer.evaluate(eval_senteval_transfer=True)\n",
        "\n",
        "        output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n",
        "        if trainer.is_world_process_zero():\n",
        "            with open(output_eval_file, \"w\") as writer:\n",
        "                logger.info(\"***** Eval results *****\")\n",
        "                for key, value in sorted(results.items()):\n",
        "                    logger.info(f\"  {key} = {value}\")\n",
        "                    writer.write(f\"{key} = {value}\\n\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def _mp_fn(index):\n",
        "    # For xla_spawn (TPUs)\n",
        "    main()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "r_DFPs46zXxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#평가모델\n"
      ],
      "metadata": {
        "id": "vSoMTMlC2StE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import io, os\n",
        "import numpy as np\n",
        "import logging\n",
        "import argparse\n",
        "from prettytable import PrettyTable\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# Set up logger\n",
        "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.DEBUG)\n",
        "\n",
        "# Set PATHs\n",
        "PATH_TO_SENTEVAL = './SentEval'\n",
        "PATH_TO_DATA = './SentEval/data'\n",
        "\n",
        "# Import SentEval\n",
        "sys.path.insert(0, PATH_TO_SENTEVAL)\n",
        "import senteval\n",
        "\n",
        "def print_table(task_names, scores):\n",
        "    tb = PrettyTable()\n",
        "    tb.field_names = task_names\n",
        "    tb.add_row(scores)\n",
        "    print(tb)\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model_name_or_path\", type=str,\n",
        "            help=\"Transformers' model name or path\")\n",
        "    parser.add_argument(\"--pooler\", type=str,\n",
        "            choices=['cls', 'cls_before_pooler', 'avg', 'avg_top2', 'avg_first_last'],\n",
        "            default='cls',\n",
        "            help=\"Which pooler to use\")\n",
        "    parser.add_argument(\"--mode\", type=str,\n",
        "            choices=['dev', 'test', 'fasttest'],\n",
        "            default='test',\n",
        "            help=\"What evaluation mode to use (dev: fast mode, dev results; test: full mode, test results); fasttest: fast mode, test results\")\n",
        "    parser.add_argument(\"--task_set\", type=str,\n",
        "            choices=['sts', 'transfer', 'full', 'na'],\n",
        "            default='sts',\n",
        "            help=\"What set of tasks to evaluate on. If not 'na', this will override '--tasks'\")\n",
        "    parser.add_argument(\"--tasks\", type=str, nargs='+',\n",
        "            default=['STS12', 'STS13', 'STS14', 'STS15', 'STS16',\n",
        "                     'MR', 'CR', 'MPQA', 'SUBJ', 'SST2', 'TREC', 'MRPC',\n",
        "                     'SICKRelatedness', 'STSBenchmark'],\n",
        "            help=\"Tasks to evaluate on. If '--task_set' is specified, this will be overridden\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Load transformers' model checkpoint\n",
        "    model = AutoModel.from_pretrained(args.model_name_or_path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Set up the tasks\n",
        "    if args.task_set == 'sts':\n",
        "        args.tasks = ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness']\n",
        "    elif args.task_set == 'transfer':\n",
        "        args.tasks = ['MR', 'CR', 'MPQA', 'SUBJ', 'SST2', 'TREC', 'MRPC']\n",
        "    elif args.task_set == 'full':\n",
        "        args.tasks = ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness']\n",
        "        args.tasks += ['MR', 'CR', 'MPQA', 'SUBJ', 'SST2', 'TREC', 'MRPC']\n",
        "\n",
        "    # Set params for SentEval\n",
        "    if args.mode == 'dev' or args.mode == 'fasttest':\n",
        "        # Fast mode\n",
        "        params = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 5}\n",
        "        params['classifier'] = {'nhid': 0, 'optim': 'rmsprop', 'batch_size': 128,\n",
        "                                         'tenacity': 3, 'epoch_size': 2}\n",
        "    elif args.mode == 'test':\n",
        "        # Full mode\n",
        "        params = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 10}\n",
        "        params['classifier'] = {'nhid': 0, 'optim': 'adam', 'batch_size': 64,\n",
        "                                         'tenacity': 5, 'epoch_size': 4}\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # SentEval prepare and batcher\n",
        "    def prepare(params, samples):\n",
        "        return\n",
        "\n",
        "    def batcher(params, batch, max_length=None):\n",
        "        # Handle rare token encoding issues in the dataset\n",
        "        if len(batch) >= 1 and len(batch[0]) >= 1 and isinstance(batch[0][0], bytes):\n",
        "            batch = [[word.decode('utf-8') for word in s] for s in batch]\n",
        "\n",
        "        sentences = [' '.join(s) for s in batch]\n",
        "\n",
        "        # Tokenization\n",
        "        if max_length is not None:\n",
        "            batch = tokenizer.batch_encode_plus(\n",
        "                sentences,\n",
        "                return_tensors='pt',\n",
        "                padding=True,\n",
        "                max_length=max_length,\n",
        "                truncation=True\n",
        "            )\n",
        "\n",
        "\n",
        "        else:\n",
        "            batch = tokenizer.batch_encode_plus(\n",
        "                sentences,\n",
        "                return_tensors='pt',\n",
        "                padding=True,\n",
        "            )\n",
        "\n",
        "        # Move to the correct device\n",
        "        for k in batch:\n",
        "            batch[k] = batch[k].to(device)\n",
        "\n",
        "        # Get raw embeddings\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch, output_hidden_states=True, return_dict=True)\n",
        "            last_hidden = outputs.last_hidden_state\n",
        "            pooler_output = outputs.pooler_output\n",
        "            hidden_states = outputs.hidden_states\n",
        "\n",
        "        # Apply different poolers\n",
        "        if args.pooler == 'cls':\n",
        "            # There is a linear+activation layer after CLS representation\n",
        "            return pooler_output.cpu()\n",
        "        elif args.pooler == 'cls_before_pooler':\n",
        "            return last_hidden[:, 0].cpu()\n",
        "        elif args.pooler == \"avg\":\n",
        "            return ((last_hidden * batch['attention_mask'].unsqueeze(-1)).sum(1) / batch['attention_mask'].sum(-1).unsqueeze(-1)).cpu()\n",
        "        elif args.pooler == \"avg_first_last\":\n",
        "            first_hidden = hidden_states[1]\n",
        "            last_hidden = hidden_states[-1]\n",
        "            pooled_result = ((first_hidden + last_hidden) / 2.0 * batch['attention_mask'].unsqueeze(-1)).sum(1) / batch['attention_mask'].sum(-1).unsqueeze(-1)\n",
        "            return pooled_result.cpu()\n",
        "        elif args.pooler == \"avg_top2\":\n",
        "            second_last_hidden = hidden_states[-2]\n",
        "            last_hidden = hidden_states[-1]\n",
        "            pooled_result = ((last_hidden + second_last_hidden) / 2.0 * batch['attention_mask'].unsqueeze(-1)).sum(1) / batch['attention_mask'].sum(-1).unsqueeze(-1)\n",
        "            return pooled_result.cpu()\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for task in args.tasks:\n",
        "        se = senteval.engine.SE(params, batcher, prepare)\n",
        "        result = se.eval(task)\n",
        "        results[task] = result\n",
        "\n",
        "    # Print evaluation results\n",
        "    if args.mode == 'dev':\n",
        "        print(\"------ %s ------\" % (args.mode))\n",
        "\n",
        "        task_names = []\n",
        "        scores = []\n",
        "        for task in ['STSBenchmark', 'SICKRelatedness']:\n",
        "            task_names.append(task)\n",
        "            if task in results:\n",
        "                scores.append(\"%.2f\" % (results[task]['dev']['spearman'][0] * 100))\n",
        "            else:\n",
        "                scores.append(\"0.00\")\n",
        "        print_table(task_names, scores)\n",
        "\n",
        "        task_names = []\n",
        "        scores = []\n",
        "        for task in ['MR', 'CR', 'SUBJ', 'MPQA', 'SST2', 'TREC', 'MRPC']:\n",
        "            task_names.append(task)\n",
        "            if task in results:\n",
        "                scores.append(\"%.2f\" % (results[task]['devacc']))\n",
        "            else:\n",
        "                scores.append(\"0.00\")\n",
        "        task_names.append(\"Avg.\")\n",
        "        scores.append(\"%.2f\" % (sum([float(score) for score in scores]) / len(scores)))\n",
        "        print_table(task_names, scores)\n",
        "\n",
        "    elif args.mode == 'test' or args.mode == 'fasttest':\n",
        "        print(\"------ %s ------\" % (args.mode))\n",
        "\n",
        "        task_names = []\n",
        "        scores = []\n",
        "        for task in ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness']:\n",
        "            task_names.append(task)\n",
        "            if task in results:\n",
        "                if task in ['STS12', 'STS13', 'STS14', 'STS15', 'STS16']:\n",
        "                    scores.append(\"%.2f\" % (results[task]['all']['spearman']['all'] * 100))\n",
        "                else:\n",
        "                    scores.append(\"%.2f\" % (results[task]['test']['spearman'].correlation * 100))\n",
        "            else:\n",
        "                scores.append(\"0.00\")\n",
        "        task_names.append(\"Avg.\")\n",
        "        scores.append(\"%.2f\" % (sum([float(score) for score in scores]) / len(scores)))\n",
        "        print_table(task_names, scores)\n",
        "\n",
        "        task_names = []\n",
        "        scores = []\n",
        "        for task in ['MR', 'CR', 'SUBJ', 'MPQA', 'SST2', 'TREC', 'MRPC']:\n",
        "            task_names.append(task)\n",
        "            if task in results:\n",
        "                scores.append(\"%.2f\" % (results[task]['acc']))\n",
        "            else:\n",
        "                scores.append(\"0.00\")\n",
        "        task_names.append(\"Avg.\")\n",
        "        scores.append(\"%.2f\" % (sum([float(score) for score in scores]) / len(scores)))\n",
        "        print_table(task_names, scores)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "XR8ZvKnu2Wv7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}